import streamlit as st
import os

st.set_page_config(page_title="åœ‹ç«‹è™å°¾ç§‘æŠ€å¤§å­¸æ©Ÿæ¢°è¨­è¨ˆå·¥ç¨‹ç³»", layout="wide")

# --- è‡ªè¨‚ CSS æ¨£å¼ ---
st.markdown("""
<style>
    /* è¨­å®šå…¨ç«™èƒŒæ™¯ç‚ºæ·±è‰² */
    .stApp {
        background-color: #0E1117;
    }

    /* å¼·åˆ¶æ‰€æœ‰ä¸»è¦æ–‡å­—å…ƒä»¶ç‚ºç™½è‰²ï¼Œä¸¦å¢åŠ è¡Œé«˜æå‡é–±è®€é«”é©— */
    h1, h2, h3, h4, h5, h6, p, div, span, label, li, .stMarkdown {
        color: #FFFFFF !important;
        line-height: 1.6 !important;
    }

    /* ä¿®æ­£ç¨‹å¼ç¢¼å€å¡Šçš„æ–‡å­—é¡è‰² */
    code {
        color: #ff4b4b !important;
    }

    /* å´é‚Šæ¬„æ–‡å­—é¡è‰² */
    .css-17lntkn {
        color: #FFFFFF !important;
    }

    /* --- GitHub é¢¨æ ¼ Expander æ¨£å¼ --- */
    .streamlit-expanderHeader {
        background-color: #161b22 !important;
        border: 1px solid #30363d !important;
        border-radius: 6px !important;
        color: #c9d1d9 !important;
        font-family: -apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji" !important;
        font-size: 14px !important;
        font-weight: 600 !important;
    }
    .streamlit-expanderHeader:hover {
        background-color: #20252c !important;
        color: #58a6ff !important;
    }
    .streamlit-expanderContent {
        background-color: #0d1117 !important;
        border: 1px solid #30363d !important;
        border-top: none !important;
        border-bottom-left-radius: 6px !important;
        border-bottom-right-radius: 6px !important;
        color: #c9d1d9 !important;
    }
    .streamlit-expanderContent code {
        background-color: transparent !important;
    }

    /* --- å´é‚Šæ¬„é¸å–®æ¨£å¼ (Gemini é¢¨æ ¼) --- */
    section[data-testid="stSidebar"] .stRadio > div {
        gap: 12px;
        padding-top: 10px;
    }
    section[data-testid="stSidebar"] .stRadio div[role="radiogroup"] > label > div:first-child {
        display: none;
    }
    section[data-testid="stSidebar"] .stRadio div[role="radiogroup"] > label {
        display: flex;
        align-items: center;
        justify-content: center !important; /* å¼·åˆ¶ Flex å®¹å™¨å…§å®¹ç½®ä¸­ */
        width: 100%;
        padding: 12px 16px;       
        border-radius: 12px;
        transition: all 0.3s ease;
        border: 1px solid transparent; 
        color: #FFFFFF !important;
        cursor: pointer;
        background-color: transparent;
        text-align: center !important; /* å¼·åˆ¶æ–‡å­—ç½®ä¸­ */
        margin: 0 auto; /* å®¹å™¨ç½®ä¸­ */
    }
    /* ç¢ºä¿ label å…§éƒ¨çš„æ–‡å­—å®¹å™¨ä¹Ÿç½®ä¸­ */
    section[data-testid="stSidebar"] .stRadio div[role="radiogroup"] > label > div[data-testid="stMarkdownContainer"] {
        text-align: center !important;
        width: 100%;
        display: flex;
        justify-content: center;
    }
    section[data-testid="stSidebar"] .stRadio div[role="radiogroup"] > label p {
        text-align: center !important;
        width: 100%;
        margin: 0;
    }

    section[data-testid="stSidebar"] .stRadio div[role="radiogroup"] > label:hover {
        background-color: rgba(255, 255, 255, 0.1);
    }
    /* è—è‰²æ¯›ç»ç’ƒé†’ç›®æ¡†é¢¨æ ¼ */
    section[data-testid="stSidebar"] .stRadio div[role="radiogroup"] > label:has(input:checked) {
        background-color: rgba(16, 83, 210, 0.5) !important;
        border: 1px solid rgba(16, 83, 210, 0);
        backdrop-filter: blur(10px);
        color: #FFFFFF !important;
        font-weight: 600;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }

    /* --- é¦–é ç½®ä¸­å°ˆç”¨æ¨£å¼ --- */
    .home-container {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        text-align: center;
        width: 100%;
        margin-top: 20px;
    }
    .home-container h1, .home-container h2, .home-container h3, .home-container p {
        text-align: center !important;
        width: 100%;
    }

</style>
""", unsafe_allow_html=True)

# --- å´é‚Šæ¬„é¸å–® ---
menu = st.sidebar.radio(
    "ç›®éŒ„",
    [
        "é¦–é ",
        "æ·±åº¦å­¸ç¿’åˆ†é¡å•é¡Œ",
        "YOLOå½±åƒè¾¨è­˜",
        "TurtleBot Burgerå¹³å°",
        "Streamlit UIè¨­è¨ˆèˆ‡è³‡æ–™å¯è¦–åŒ–",
        "RLå»ºæ¨¡èˆ‡è¨“ç·´"
    ],
    label_visibility="collapsed",
    key="main_menu"
)


# --- Helper Function: å®‰å…¨é¡¯ç¤ºåª’é«” ---
def show_media(path, media_type='image', caption="", width=None):
    if os.path.exists(path):
        if media_type == 'image':
            st.image(path, caption=caption, width=width, use_container_width=(width is None))
        elif media_type == 'video':
            st.video(path)
    else:
        st.warning(f"âš ï¸æ‰¾ä¸åˆ°æª”æ¡ˆ: {path} (è«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦å·²æ”¾å…¥è³‡æ–™å¤¾)")


# --- é é¢å…§å®¹ ---

if menu == "é¦–é ":
    st.markdown("""
        <div class="home-container">
            <h1>114(ä¸Š)å­¸å¹´åº¦ã€æ™ºæ…§æ©Ÿæ¢°è¨­è¨ˆã€èª²ç¨‹æœŸæœ«å ±å‘Š</h1>
            <h2>ROSè‡ªä¸»ç§»å‹•å¹³å°èˆ‡AIæ•´åˆä¹‹ç ”ç©¶</h2>
            <br>
            <h3>æŒ‡å°è€å¸«ï¼šå‘¨æ¦®æº</h3>
            <h3>ç­ç´šï¼šç¢©è¨­è¨ˆä¸€ç”²</h3>
            <h3>çµ„åˆ¥ï¼šç¬¬ä¸€çµ„</h3>
            <h3>çµ„å“¡ï¼š11473132 é™³å¨èªŒ
            <h3>     11473107 ç´€é–”ç¿”
            <h3>     11473143 æœ±ç‹é»ƒ</h3>           
            <br>
            <p style='font-size: 1.1em;'>æ­¡è¿ä¾†åˆ°æ™ºæ…§æ©Ÿæ¢°è¨­è¨ˆèª²ç¨‹æœŸæœ«å ±å‘Šã€‚è«‹å¾å·¦å´é¸å–®é¸æ“‡è¦æŸ¥çœ‹çš„å¯¦é©—é …ç›®ã€‚</p>
        </div>
    """, unsafe_allow_html=True)

    st.markdown("<br>", unsafe_allow_html=True)

    col1, col2, col3 = st.columns([1, 300, 1])
    with col2:
        show_media("img/Turtlebot/first.jpg", caption="Turtlebot3")


elif menu == "æ·±åº¦å­¸ç¿’åˆ†é¡å•é¡Œ":
    st.title("æ·±åº¦å­¸ç¿’åˆ†é¡å•é¡Œ")
    st.markdown("---")

    st.header("ä¸€ã€å¯¦ä½œéç¨‹ï¼š")

    st.subheader('æ­¥é©Ÿ 1ï¼šPC ç«¯å•Ÿå‹• ROS Master')
    st.caption("ç”¨é€”ï¼šä½œç‚ºæ•´å€‹ç³»çµ±çš„ ROS ä¸­æ¨ï¼Œè² è²¬ç®¡ç†æ‰€æœ‰ ROS ç¯€é»èˆ‡ topic")
    show_media("img/rviz/1.jpg")
    code = """
export ROS_MASTER_URI = http://192.168.1.203:11311
export ROS_IP = 192.168.1.203
roscore"""
    st.code(code, language="bash")

    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 2ï¼šé€£ç·šè‡³ TurtleBot3 Burgerï¼ˆRaspberry Piï¼‰')
    st.caption("ç”¨é€”ï¼šé ç«¯æ§åˆ¶ TurtleBot3ï¼Œå•Ÿå‹•æ©Ÿå™¨äººç«¯ç¯€é»")
    show_media("img/rviz/2.jpg")
    st.code("ssh pi@192.168.1.199", language="bash")

    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 3ï¼šTurtleBot3 ç«¯ï¼šBringupï¼ˆåº•å±¤ç³»çµ±å•Ÿå‹•ï¼‰')
    st.caption("ç”¨é€”ï¼šå•Ÿå‹•é¦¬é”ã€é›·å°„æ„Ÿæ¸¬å™¨ã€TF æ¶æ§‹ï¼Œä½¿æ©Ÿå™¨äººå¯æ¥æ”¶ /cmd_vel")
    show_media("img/rviz/3.jpg")
    code = """
export TURTLEBOT3_MODEL = burger
export ROS_MASTER_URI = http://192.168.1.203:11311
export ROS_IP = 192.168.1.199 
roslaunch turtlebot3_bringup turtlebot3_robot.launch"""
    st.code(code, language="bash")

    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 4ï¼šTurtleBot3 ç«¯ï¼šå•Ÿå‹• USB Camera')
    st.caption("ç”¨é€”ï¼šå–å¾—å³æ™‚å½±åƒä¸¦ç™¼å¸ƒç‚º ROS topic")
    show_media("img/rviz/usb.jpg")
    code = """
export ROS_MASTER_URI = http://192.168.1.203:11311
export ROS_IP=192.168.1.199
roslaunch usb_cam usb_cam-test.launch """
    st.code(code, language="bash")

    st.markdown('**æˆåŠŸå¾Œå½±åƒæœƒç™¼å¸ƒè‡³**')
    st.code("/usb_cam/image_raw", language="bash")

    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 5ï¼šPC ç«¯éµç›¤æ§åˆ¶æ¸¬è©¦')
    st.caption("ç”¨é€”ï¼šç¢ºèª /cmd_vel æ§åˆ¶é€šé“æ­£å¸¸ï¼Œé¿å…å¾ŒçºŒèª¤åˆ¤ç‚ºæ¨¡å‹éŒ¯èª¤")
    show_media("img/rviz/5.jpg")
    st.header("æˆåŠŸç•«é¢")
    show_media("img/rviz/6.jpg")
    code = """
export TURTLEBOT3_MODEL=burger
export ROS_MASTER_URI=http://192.168.1.203:11311
export ROS_IP=192.168.1.203
roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch"""
    st.code(code, language="bash")

    st.markdown("---")

    st.title("äºŒã€å½±åƒè¾¨è­˜èˆ‡æ§åˆ¶æ ¸å¿ƒç¨‹å¼ç¢¼")
    st.markdown("""
    ä»¥ä¸‹ç‚ºå®Œæ•´å¯åŸ·è¡Œç‰ˆæœ¬ï¼Œæ­¤ç¨‹å¼è² è²¬ï¼š
    1. è¨‚é–± TurtleBot3 ç›¸æ©Ÿå½±åƒ
    2. é€²è¡Œå‰é€² / å¾Œé€€å½±åƒè¾¨è­˜
    3. ç™¼å¸ƒ `/cmd_vel` æ§åˆ¶ Burger ç§»å‹•
    """)
    show_media("img/rviz/7.jpg")
    code = """
source /opt/ros/noetic/setup.bash
source ~/mde_ws/devel_isolated/setup.bash
export ROS_MASTER_URI=http://192.168.1.203:11311
export ROS_IP=192.168.1.203
rosrun gesture_control gesture_cmd_vel.py"""
    st.code(code, language="bash")
    st.header("æˆåŠŸç•«é¢")
    show_media("img/rviz/8.jpg")



    code_gesture_cmd_vel = """
#!/usr/bin/env python3
import rospy
import rospkg
from sensor_msgs.msg import Image
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
import cv2
import numpy as np
from keras.models import load_model

# =========================
# ROS node initialization
# =========================
rospy.init_node("gesture_control_node")

# =========================
# Load model & labels
# =========================
rospack = rospkg.RosPack()
pkg_path = rospack.get_path("gesture_control")

model_path = pkg_path + "/model/keras_model.h5"
label_path = pkg_path + "/model/labels.txt"

model = load_model(model_path, compile=False)
class_names = open(label_path, "r").readlines()

# =========================
# ROS publisher / subscriber
# =========================
bridge = CvBridge()
cmd_pub = rospy.Publisher("/cmd_vel", Twist, queue_size=10)

CONF_TH = 0.8   # ä¿¡å¿ƒå€¼é–€æª»

# =========================
# Image callback function
# =========================
def image_cb(msg):
    frame = bridge.imgmsg_to_cv2(msg, desired_encoding="bgr8")

    # Resize & normalize
    img = cv2.resize(frame, (224, 224))
    img = np.asarray(img, dtype=np.float32).reshape(1,224,224,3)
    img = (img / 127.5) - 1

    # Predict
    pred = model.predict(img, verbose=0)
    idx = np.argmax(pred)
    conf = pred[0][idx]
    label = class_names[idx].strip()

    twist = Twist()

    if conf > CONF_TH:
        if "forward" in label:
            twist.linear.x = 0.2      # å‰é€²
        elif "back" in label:
            twist.linear.x = -0.2     # å¾Œé€€

        cmd_pub.publish(twist)

    # Debug display
    cv2.putText(frame, f"{label} {conf:.2f}",
                (10,30), cv2.FONT_HERSHEY_SIMPLEX,
                1, (0,255,0), 2)
    cv2.imshow("Gesture Control", frame)
    cv2.waitKey(1)

# =========================
# Subscriber
# =========================
rospy.Subscriber("/usb_cam/image_raw", Image, image_cb)
rospy.spin()
"""
    with st.expander("é»æ“Šè¤‡è£½ç›¸é—œç¨‹å¼ç¢¼ (gesture_cmd_vel.py)"):
        st.code(code_gesture_cmd_vel, language="python")

    st.markdown("---")

    st.header('å½±åƒè¾¨è­˜æƒ…å½¢')
    st.caption("ç”¨é€”ï¼šTechable Machine è³‡æ–™é›†ç¨‹å¼è¾¨è­˜æƒ…æ³")
    show_media("img/rviz/T.jpg")
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.markdown('#### (a) å·¦è½‰')
        show_media("img/rviz/left.jpg")
    with col2:
        st.markdown('#### (b) å³è½‰')
        show_media("img/rviz/right.jpg")
    with col3:
        st.markdown('#### (c) å‰é€²')
        show_media("img/rviz/up.jpg")
    with col4:
        st.markdown('#### (d) å¾Œé€€')
        show_media("img/rviz/down.jpg")
    with col5:
        st.markdown('#### (e) STOP')
        show_media("img/rviz/stop.jpg")

    st.markdown("---")

    st.header("äºŒã€çµæœå±•ç¤º")
    st.markdown('### æˆåŠŸä½¿ Turtlebot3 ä¾ç®­é ­æ–¹å‘åšå‡ºç›¸æ‡‰å‹•ä½œ')
    st.subheader('(A) å·¦è½‰')
    show_media("img/rviz/left.mp4", "video")
    st.markdown("---")
    st.subheader('(B) å³è½‰')
    show_media("img/rviz/right.mp4", "video")
    st.markdown("---")
    st.subheader('(C) å‰é€²')
    show_media("img/rviz/up.mp4", "video")
    st.markdown("---")
    st.subheader('(D) å¾Œé€€')
    show_media("img/rviz/down.mp4", "video")
    st.markdown("---")
    st.subheader('(E) STOP')
    show_media("img/rviz/stop.mp4", "video")

elif menu == "YOLOå½±åƒè¾¨è­˜":
    st.title("YOLO å½±åƒè¾¨è­˜å¯¦ä½œæ­¥é©Ÿ")
    st.markdown("---")

    st.subheader("æ­¥é©Ÿ 1ï¼šå½±åƒæ•¸æ“šæ”¶é›† (Data images collection)")
    show_media("img/yolo/yolo1.png", caption="å½±åƒæ”¶é›†ç¤ºæ„åœ–")

    st.markdown("---")

    st.subheader("æ­¥é©Ÿ 2ï¼šæ¨™è¨»é¡åˆ¥ (Annotating Labels Classes - Roboflow)")
    show_media("img/yolo/yolo2.png", caption="Roboflow æ¨™è¨»ç•«é¢")

    st.markdown("---")

    st.subheader("æ­¥é©Ÿ 3ï¼šå»ºç«‹ Mushroom è³‡æ–™é›† (Roboflow)")
    st.markdown("ç¸½æ•¸: 124 å¼µ (è¨“ç·´é›†: 111, é©—è­‰é›†: 9, æ¸¬è©¦é›†: 4)")
    st.markdown("[ğŸ”— é»æ“Šå‰å¾€ Roboflow å°ˆæ¡ˆé€£çµ](https://app.roboflow.com/mushrooms-object-detection/king_mushroom/3)")
    show_media("img/yolo/yolo3.png", caption="è³‡æ–™é›†åˆ†ä½ˆåœ–")

    st.markdown("---")

    st.subheader("æ­¥é©Ÿ 4ï¼šè¨“ç·´è³‡æ–™é›† (Google Colab é«˜æ•ˆèƒ½ GPU T4)")
    show_media("img/yolo/yolo4.png", caption="Colab è¨“ç·´éç¨‹")

    st.markdown("---")

    st.subheader("æ­¥é©Ÿ 5ï¼šåœ¨ Ubuntu Linux ä¸Šé€²è¡Œ Mushroom åµæ¸¬")

    st.markdown("**åœ–ä¸€ï¼šé–‹å•Ÿæ”å½±æ©Ÿä¸¦æª¢æŸ¥è£ç½®**")
    show_media("img/yolo/yolo5.png", caption="é–‹å•Ÿ webcam")

    st.markdown("**åœ–äºŒï¼šåŸ·è¡Œ Python ç¨‹å¼**")
    show_media("img/yolo/yolo6.png", caption="Step 2: Run python file")

    code_mushroom = """
import cv2
from ultralytics import solutions

# 1. model
VIDEO_SOURCE = 0            # 0: Camera laptop
MODEL_PATH = "best.pt"      # Model .pt
CONF_THRESHOLD = 0.8        # threshold
LINE_POSITION = 0.66        # line (right side)

# 2. CAMERA 
cap = cv2.VideoCapture(VIDEO_SOURCE)
assert cap.isOpened(), "Can not open Camera"
w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))

# Line zone (right side)
line_x = int(w * LINE_POSITION)
line_points = [(line_x, 0), (line_x, h)]

# (OBJECT COUNTER)
counter = solutions.ObjectCounter(
    show=False,              # show video output
    region=line_points,     # line
    model=MODEL_PATH,       # Link model
    conf=CONF_THRESHOLD,    # threshold
    line_width=2,
    # classes=[0]           
)

print("Running... Press 'q' to exit.")

while cap.isOpened():
    success, im0 = cap.read()
    if not success: break
    result = counter.process(im0)
    im_output = result.plot_im

    # show
    cv2.imshow("Mushroom Counter Optimized", im_output)
    
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
"""
    with st.expander("é»æ“Šè¤‡è£½ç›¸é—œç¨‹å¼ç¢¼ (run_mushroom.py)"):
        st.code(code_mushroom, language="python")

    st.markdown("**åœ–ä¸‰ï¼šåµæ¸¬çµæœç•«é¢**")
    show_media("img/yolo/yolo7.png", caption="åµæ¸¬åŸ·è¡Œä¸­")

    code_video_counter = """
import cv2

from ultralytics import solutions

cap = cv2.VideoCapture("test_mushroom.mp4")
assert cap.isOpened(), "Error reading video file"

# region_points = [(20, 400), (1080, 400)]                                      # line counting
#region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]  # rectangular region
region_points = [[691, 113], [959, 115], [957, 535], [700, 528]]   # polygon region

# Video writer
w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))
video_writer = cv2.VideoWriter("object_counting.avi", cv2.VideoWriter_fourcc(*"mp4v"), fps, (w, h))

# Initialize object counter object
counter = solutions.ObjectCounter(
    show=True,  # display the output
    region=region_points,  # pass region points
    model="best.pt",  # model="yolo11n-obb.pt" for object counting with OBB model.
    # classes=[0, 2],  # count specific classes, e.g., person and car with the COCO pretrained model.
    # tracker="botsort.yaml",  # choose trackers, e.g., "bytetrack.yaml"
)

# Process video
while cap.isOpened():
    success, im0 = cap.read()

    if not success:
        print("Video frame is empty or processing is complete.")
        break

    results = counter(im0)

    # print(results)  # access the output

    video_writer.write(results.plot_im)  # write the processed frame.

cap.release()
video_writer.release()
cv2.destroyAllWindows()  # destroy all opened windows
"""
    with st.expander("é»æ“Šè¤‡è£½ç›¸é—œç¨‹å¼ç¢¼ (counting_mushroom.py)"):
        st.code(code_video_counter, language="python")

    st.markdown("---")

    st.subheader("æ­¥é©Ÿ 6ï¼šä½¿ç”¨ Webcam é€²è¡Œåµæ¸¬èˆ‡è¨ˆæ•¸")
    show_media("img/yolo/yolo1.mp4", "video")

    st.markdown("---")

    st.subheader("æ­¥é©Ÿ 7ï¼šä½¿ç”¨åœ–ç‰‡/å½±ç‰‡é€²è¡Œåµæ¸¬èˆ‡è¨ˆæ•¸")
    show_media("img/yolo/yolo2.mp4", "video")


elif menu == "RLå»ºæ¨¡èˆ‡è¨“ç·´":
    st.title("RL å»ºæ¨¡èˆ‡è¨“ç·´")
    # æ–°å¢å­é¸å–®åˆ†æ”¯
    rl_nav = st.sidebar.radio(
        "RL é¸å–®",
        ["ç³»çµ±æ¶æ§‹èˆ‡å¯¦ä½œ", "çå‹µå‡½æ•¸è©³ç´°è§£èªª"],
        label_visibility="collapsed",
        key="rl_menu"
    )

    if rl_nav == "ç³»çµ±æ¶æ§‹èˆ‡å¯¦ä½œ":
        st.header("ä¸€ã€ç ”ç©¶å‹•æ©Ÿèˆ‡å°ˆæ¡ˆç›®æ¨™")
        st.subheader('æ¦‚è¿°')
        st.markdown("""
        Three_link_rl å°ˆæ¡ˆæ—¨åœ¨å»ºç«‹ä¸€å¥—åŸºæ–¼æ·±åº¦å¼·åŒ–å­¸ç¿’ï¼ˆDeep Reinforcement Learning, DRLï¼‰ä¹‹ä¸‰é€£æ¡¿å¹³é¢æ©Ÿæ¢°æ‰‹è‡‚æ§åˆ¶ç³»çµ±ã€‚
        ç³»çµ±é€éå­¸ç¿’å‹æ§åˆ¶ç­–ç•¥ï¼Œä½¿ä¸‰é€£æ¡¿æ©Ÿæ¢°è‡‚èƒ½åœ¨æœªçŸ¥ç›®æ¨™ä½ç½®æ¢ä»¶ä¸‹ï¼Œè‡ªä¸»è¿½è¹¤ä¸¦ç©©å®šåœç•™æ–¼ç›®æ¨™å€åŸŸï¼ŒåŒæ™‚å…·å‚™è‰¯å¥½çš„å‹•ä½œå¹³é †æ€§èˆ‡æ§åˆ¶ç©©å®šåº¦ã€‚
        æœ¬å°ˆæ¡ˆä¸ä¾è³´å‚³çµ±è§£æå¼é€†é‹å‹•å­¸ï¼ˆInverse Kinematics, IKï¼‰ï¼Œè€Œæ˜¯æ¡ç”¨æ¨¡å‹è‡ªç”±ï¼ˆmodel-freeï¼‰å¼·åŒ–å­¸ç¿’æ–¹æ³•ï¼Œä½¿ç³»çµ±å…·å‚™è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›èˆ‡å»¶å±•æ€§ã€‚
        """)
        st.markdown("---")

        st.header("äºŒã€ç³»çµ±æ¶æ§‹æ¦‚è¿°")
        st.markdown("æœ¬å°ˆæ¡ˆç³»çµ±ç”±ä¸‰å€‹æ ¸å¿ƒæ¨¡çµ„çµ„æˆï¼Œå½¢æˆæ¨™æº– Agentâ€“Environment äº’å‹•é–‰ç’°ï¼š")
        code = """
three_link_rl/
â”œâ”€â”€ env.py   # ç’°å¢ƒå»ºæ¨¡ï¼ˆMDP + æ§åˆ¶ç©©å®šåŒ–ï¼‰
â”œâ”€â”€ rl.py    # DDPG Actorâ€“Critic å­¸ç¿’å™¨
â””â”€â”€ main.py  # è¨“ç·´èˆ‡æ¸¬è©¦æµç¨‹æ§åˆ¶ """
        st.code(code, language="bash")

        st.markdown("""
        <div style="font-size: 16px;">
            <ul>
                <li><strong>env.py</strong>ï¼šè² è²¬æ©Ÿæ¢°çµæ§‹å»ºæ¨¡ã€ç‹€æ…‹å®šç¾©ã€çå‹µè¨­è¨ˆèˆ‡æ§åˆ¶å¾Œè™•ç†</li>
                <li><strong>rl.py</strong>ï¼šå¯¦ä½œ DDPG å¼·åŒ–å­¸ç¿’æ¼”ç®—æ³•</li>
                <li><strong>main.py</strong>ï¼šè² è²¬è¨“ç·´æµç¨‹èˆ‡æ¨è«–å±•ç¤º</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

        st.markdown("---")

        st.header("ä¸‰ã€å¼·åŒ–å­¸ç¿’ç’°å¢ƒè¨­è¨ˆ")
        st.subheader("3.1 ä¸‰é€£æ¡¿æ©Ÿæ¢°æ‰‹è‡‚å»ºæ¨¡")
        st.markdown("ç³»çµ±æ¨¡æ“¬ä¸€å€‹å¹³é¢ä¸‰é€£æ¡¿æ©Ÿæ¢°æ‰‹è‡‚ï¼Œæ¯å€‹é—œç¯€çš†ç‚ºæ—‹è½‰é—œç¯€ï¼ˆRevolute Jointï¼‰ï¼Œé€éå‰å‘é‹å‹•å­¸è¨ˆç®—æœ«ç«¯ä½ç½®ï¼š")
        code = """
def _get_joint_positions(self):
    p0 = self.base
    p1 = p0 + [cos(Î¸1), sin(Î¸1)] * l1
    p2 = p1 + [cos(Î¸1+Î¸2), sin(Î¸1+Î¸2)] * l2
    p3 = p2 + [cos(Î¸1+Î¸2+Î¸3), sin(Î¸1+Î¸2+Î¸3)] * l3
    return p0, p1, p2, p3 """
        st.code(code, language="python")

        st.markdown("---")

        st.subheader("3.2 ç‹€æ…‹ç©ºé–“ï¼ˆState Spaceï¼‰")
        st.markdown(
            "æ…‹å‘é‡å…± 15 ç¶­ï¼ŒåŒ…å«ï¼šæœ«ç«¯èˆ‡ç›®æ¨™ä¹‹ç›¸å°å‘é‡èˆ‡è·é›¢æœ«ç«¯é€Ÿåº¦ï¼ˆå¹³æ»‘æ€§è©•ä¼°ï¼‰æ˜¯å¦é€²å…¥ç›®æ¨™å€åŸŸçš„ç‹€æ…‹è¨˜æ†¶é—œç¯€è§’åº¦ä¹‹ sin / cos è¡¨ç¤ºå‰ä¸€æ™‚é–“æ­¥çš„æ§åˆ¶å‹•ä½œ")
        code = """
state = [
    dist_vec(2), dist(1),
    ee_vel(2), on_goal(1),
    cos(theta)(3), sin(theta)(3),
    prev_action(3)
]"""
        st.code(code, language="python")
        st.caption("æ­¤è¨­è¨ˆåŒæ™‚å…¼é¡§å¹¾ä½•é—œä¿‚ã€å‹•æ…‹ç‰¹æ€§èˆ‡æ§åˆ¶é€£çºŒæ€§ã€‚")

        st.markdown("---")

        st.subheader("3.3 å‹•ä½œç©ºé–“ï¼ˆAction Spaceï¼‰")
        code = """
action = np.clip(action, -0.5, 0.5)
self.arm_info[:, 1] += action * dt"""
        st.code(code, language="python")
        st.caption("æ¯ä¸€ç¶­å‹•ä½œå°æ‡‰ä¸€å€‹é—œç¯€è§’é€Ÿåº¦å¢é‡ï¼Œé©ç”¨æ–¼é€£çºŒæ§åˆ¶å‹æ¼”ç®—æ³•ã€‚")

        st.markdown("---")

        st.subheader("3.4 çå‹µå‡½æ•¸è¨­è¨ˆï¼ˆReward Functionï¼‰")
        st.markdown("æœ¬å°ˆæ¡ˆæ¡ç”¨è·é›¢å·®åˆ†å¼çå‹µå‡½æ•¸ï¼š")
        st.latex(r"r = (d_{t-1} - d_t) \times 20.0")
        st.markdown("""
        <div style="font-size: 16px;">
            <strong>ä¸¦é¡å¤–åŠ å…¥ï¼š</strong>
            <ul>
                <li>æœ«ç«¯é€Ÿåº¦æ‡²ç½°ï¼ˆé¿å…é«˜é »æŠ–å‹•ï¼‰</li>
                <li>è§’é€Ÿåº¦æ‡²ç½°ï¼ˆé¿å…å¤šé¤˜å‹•ä½œï¼‰</li>
                <li>ç›®æ¨™å‘½ä¸­æŒçºŒçå‹µï¼ˆé¼“å‹µç©©å®šåœç•™ï¼‰</li>
            </ul>
            <p>æ­¤è¨­è¨ˆèƒ½æœ‰æ•ˆé¿å… sparse reward å•é¡Œï¼Œä¸¦æå‡å­¸ç¿’ç©©å®šåº¦ã€‚</p>
        </div>
        """, unsafe_allow_html=True)

        st.markdown("---")

        st.subheader("3.5 æ§åˆ¶å¾Œè™•ç†èˆ‡æŠ–å‹•æŠ‘åˆ¶")
        st.markdown("ç‚ºè§£æ±ºå¼·åŒ–å­¸ç¿’æ§åˆ¶ä¸­å¸¸è¦‹çš„é«˜é »æŠ–å‹•å•é¡Œï¼Œæœ¬ç ”ç©¶æ–¼ç’°å¢ƒå±¤åŠ å…¥æ§åˆ¶å¾Œè™•ç†ï¼š")
        code = """
# å‹•ä½œå¹³æ»‘ï¼ˆä½é€šæ¿¾æ³¢ï¼‰
action = alpha * prev_action + (1 - alpha) * action

# å¾®å°è§’é€Ÿåº¦æ­»å€
action[abs(action) < deadband] = 0

# ç›®æ¨™å€ soft stop
if dist < goal_radius:
    action *= 0.2"""
        st.code(code, language="python")
        st.caption("æ­¤ä½œæ³•ä½¿æœ€çµ‚è¡Œç‚ºå‘ˆç¾æ¥è¿‘å·¥æ¥­ç´šæ©Ÿæ¢°æ‰‹è‡‚ä¹‹å¹³é †æ§åˆ¶æ•ˆæœ")
        code = """
# env.py
import numpy as np
import pyglet


class ArmEnv(object):
    viewer = None
    dt = 0.1
    action_bound = [-1, 1]
    action_dim = 3
    state_dim = 15  # dist_vec(2) + dist(1) + ee_vel(2) + on_goal(1) + cos(3)+sin(3) + prev_action(3)

    def __init__(self, allow_mouse_goal=False, random_goal_on_reset=True):
        self.W, self.H = 400, 400
        self.base = np.array([200., 200.], dtype=np.float32)

        self.goal = {'x': 100., 'y': 100., 'l': 50.0}

        self.allow_mouse_goal = allow_mouse_goal
        self.random_goal_on_reset = random_goal_on_reset

        self.arm_info = np.zeros((3, 2), dtype=np.float32)
        self.arm_info[:, 0] = [100.0, 100.0, 50.0]
        self.arm_info[:, 1] = np.random.uniform(0, 2*np.pi, size=3).astype(np.float32)

        self.on_goal = 0
        self.prev_action = np.zeros(3, dtype=np.float32)
        self.prev_ee_pos = self._get_ee_pos()
        self.prev_dist = self._dist_to_goal(self.prev_ee_pos)

    # ===================== Kinematics =====================
    def _get_joint_positions(self):
        tr = self.arm_info[:, 1]
        l = self.arm_info[:, 0]

        p0 = self.base.copy()
        p1 = p0 + np.array([np.cos(tr[0]), np.sin(tr[0])]) * l[0]
        p2 = p1 + np.array([np.cos(tr[0]+tr[1]), np.sin(tr[0]+tr[1])]) * l[1]
        p3 = p2 + np.array([np.cos(tr[0]+tr[1]+tr[2]), np.sin(tr[0]+tr[1]+tr[2])]) * l[2]
        return p0, p1, p2, p3

    def _get_ee_pos(self):
        return self._get_joint_positions()[-1]

    def _dist_to_goal(self, ee_pos):
        g = np.array([self.goal['x'], self.goal['y']], dtype=np.float32)
        return float(np.linalg.norm(ee_pos - g))

    def _get_state(self):
        tr = self.arm_info[:, 1]
        ee = self._get_ee_pos()
        g = np.array([self.goal['x'], self.goal['y']], dtype=np.float32)

        dist_vec = (g - ee) / 200.0
        dist = np.linalg.norm(g - ee) / 200.0
        ee_vel = (ee - self.prev_ee_pos) / 20.0
        touch = 1.0 if self.on_goal > 0 else 0.0
        c = np.cos(tr)
        s = np.sin(tr)

        return np.concatenate([
            dist_vec, [dist], ee_vel, [touch], c, s, self.prev_action
        ]).astype(np.float32)

    # ===================== RL Interface =====================
    def step(self, action):
        done = False

        # --- clip ---
        action = np.clip(action, -0.5, 0.5).astype(np.float32)

        ee = self._get_ee_pos()
        dist = self._dist_to_goal(ee)

        # === 1ï¸âƒ£ è·é›¢è‡ªé©æ‡‰å¹³æ»‘ï¼ˆè¶Šè¿‘è¶Šç©©ï¼‰===
        if dist < self.goal['l']:
            alpha = 0.9
        else:
            alpha = 0.75
        action = alpha * self.prev_action + (1 - alpha) * action

        # === 2ï¸âƒ£ å¾®å°è§’é€Ÿåº¦æ­»å€ï¼ˆæŠ–å‹•æ®ºæ‰‹ï¼‰===
        rate_deadband = 0.01
        action[np.abs(action) < rate_deadband] = 0.0

        # === 3ï¸âƒ£ ç›®æ¨™å€ soft stop ===
        if dist < self.goal['l'] * 0.5:
            action *= 0.2

        # --- update joint ---
        self.arm_info[:, 1] += action * self.dt
        self.arm_info[:, 1] %= (2 * np.pi)

        ee_new = self._get_ee_pos()
        dist_new = self._dist_to_goal(ee_new)

        # --- reward ---
        r = (self.prev_dist - dist_new) * 20.0
        self.prev_dist = dist_new

        ee_vel = np.linalg.norm(ee_new - self.prev_ee_pos)
        r -= 0.01 * ee_vel
        r -= 0.001 * np.sum(np.abs(action))

        if dist_new < self.goal['l']:
            r += 5.0
            self.on_goal += 1
            if self.on_goal >= 50:
                done = True
                r += 50.0
        else:
            self.on_goal = 0

        self.prev_action = action.copy()
        self.prev_ee_pos = ee_new.copy()

        return self._get_state(), float(r), done

    def reset(self):
        self.on_goal = 0
        self.prev_action = np.zeros(3, dtype=np.float32)
        self.arm_info[:, 1] = np.random.uniform(0, 2*np.pi, size=3).astype(np.float32)

        if self.random_goal_on_reset and not self.allow_mouse_goal:
            margin = 70
            self.goal['x'] = float(np.random.uniform(margin, self.W - margin))
            self.goal['y'] = float(np.random.uniform(margin, self.H - margin))

        self.prev_ee_pos = self._get_ee_pos()
        self.prev_dist = self._dist_to_goal(self.prev_ee_pos)

        return self._get_state()

    def render(self):
        if self.viewer is None:
            self.viewer = Viewer(self.arm_info, self.goal, self.base, self.allow_mouse_goal, self.W, self.H)
        self.viewer.render()

    def sample_action(self):
        return np.random.uniform(-0.5, 0.5, size=3).astype(np.float32)


# ===================== Viewer =====================
class Viewer(pyglet.window.Window):
    bar_thc = 5

    def __init__(self, arm_info, goal, base, allow_mouse_goal, W, H):
        super().__init__(width=W, height=H, caption='Arm')
        pyglet.gl.glClearColor(1, 1, 1, 1)

        self.arm_info = arm_info
        self.goal_info = goal
        self.base = base
        self.allow_mouse_goal = allow_mouse_goal
        self.batch = pyglet.graphics.Batch()

        self.point = self.batch.add(4, pyglet.gl.GL_QUADS, None,
            ('v2f', [0]*8), ('c3B', (86, 109, 249)*4))

        self.arm1 = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', [0]*8), ('c3B', (249, 86, 86)*4))
        self.arm2 = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', [0]*8), ('c3B', (249, 86, 86)*4))
        self.arm3 = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', [0]*8), ('c3B', (249, 86, 86)*4))

    def render(self):
        self._update()
        self.switch_to()
        self.dispatch_events()
        self.clear()
        self.batch.draw()
        self.flip()

    def _update(self):
        x, y, l = self.goal_info['x'], self.goal_info['y'], self.goal_info['l']
        self.point.vertices = [
            x-l/2, y-l/2, x-l/2, y+l/2, x+l/2, y+l/2, x+l/2, y-l/2
        ]

        a1l, a2l, a3l = self.arm_info[:, 0]
        a1r, a2r, a3r = self.arm_info[:, 1]
        p0 = self.base
        p1 = p0 + np.array([np.cos(a1r), np.sin(a1r)]) * a1l
        p2 = p1 + np.array([np.cos(a1r+a2r), np.sin(a1r+a2r)]) * a2l
        p3 = p2 + np.array([np.cos(a1r+a2r+a3r), np.sin(a1r+a2r+a3r)]) * a3l

        def quad(pA, pB):
            v = pB - pA
            v = v / (np.linalg.norm(v) + 1e-6)
            n = np.array([-v[1], v[0]]) * self.bar_thc
            return np.concatenate([pA-n, pA+n, pB+n, pB-n]).astype(int).tolist()

        self.arm1.vertices = quad(p0, p1)
        self.arm2.vertices = quad(p1, p2)
        self.arm3.vertices = quad(p2, p3)

    def on_mouse_motion(self, x, y, dx, dy):
        if self.allow_mouse_goal:
            self.goal_info['x'] = float(x)
            self.goal_info['y'] = float(y)"""

        with st.expander("é»æ“Šè¤‡è£½å®Œæ•´ç¨‹å¼ç¢¼ (env.py)"):
            st.code(code, language="python")

        st.markdown("---")

        st.header("å››ã€å­¸ç¿’å™¨è¨­è¨ˆ")
        st.subheader("4.1 æ¼”ç®—æ³•é¸æ“‡ï¼šDDPG")
        st.markdown("""
        æœ¬å°ˆæ¡ˆæ¡ç”¨ **Deep Deterministic Policy Gradient (DDPG)**ï¼Œé©ç”¨æ–¼ï¼š
        * é€£çºŒå‹•ä½œç©ºé–“
        * é«˜ç¶­éç·šæ€§æ§åˆ¶å•é¡Œ

        **DDPG æ¶æ§‹åŒ…å«ï¼š**
        * Actor Networkï¼šè¼¸å‡ºæ§åˆ¶å‹•ä½œ
        * Critic Networkï¼šä¼°è¨ˆ Q-value
        * Target Network èˆ‡ Soft Update
        * Replay Buffer æ‰“ç ´è³‡æ–™ç›¸é—œæ€§
        """)
        code = """
a_loss = -tf.reduce_mean(q)
td_error = mse(r + Î³ * Q_target, Q_eval)"""
        st.code(code, language="python")

        st.markdown("---")

        st.subheader("4.2 åƒè€ƒä¾†æºï¼ˆè«ç…© Pythonï¼‰")
        st.markdown("""
        æœ¬å°ˆæ¡ˆ DDPG æ¶æ§‹èˆ‡è¨“ç·´æµç¨‹ä¸»è¦åƒè€ƒï¼š
        * **è«ç…©ï¼ˆMofanï¼‰Python å¼·åŒ–å­¸ç¿’æ•™å­¸ç³»åˆ—** â€“ DDPG å¯¦ä½œæ¶æ§‹

        åƒè€ƒé‡é»åŒ…æ‹¬ï¼š
        * Actorâ€“Critic åˆ†é›¢å¼ç¶²è·¯æ¶æ§‹
        * Replay Buffer è¨­è¨ˆæ–¹å¼
        * Target Network èˆ‡ Soft Update æ©Ÿåˆ¶

        **æœ¬ç ”ç©¶é€²ä¸€æ­¥é‡å°æ©Ÿæ¢°æ‰‹è‡‚æ§åˆ¶å•é¡Œé€²è¡Œä»¥ä¸‹æ”¹è‰¯ï¼š**
        * æ›´é«˜ç¶­ä¸”é€£çºŒçš„ç‹€æ…‹è¨­è¨ˆ
        * è·é›¢å·®åˆ†å‹ reward
        * æ§åˆ¶å¾Œè™•ç†æŠ‘åˆ¶æŠ–å‹•ï¼ˆè«ç…©åŸå§‹ç¯„ä¾‹æœªåŒ…å«ï¼‰     
        """)

        code = """
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np

#####################  hyper parameters  ####################

LR_A = 0.001    # learning rate for actor
LR_C = 0.001    # learning rate for critic
GAMMA = 0.9     # reward discount
TAU = 0.01      # soft replacement
MEMORY_CAPACITY = 30000
BATCH_SIZE = 32


class DDPG(object):
    def __init__(self, a_dim, s_dim, a_bound,):
        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)
        self.pointer = 0
        self.memory_full = False
        self.sess = tf.Session()
        self.a_replace_counter, self.c_replace_counter = 0, 0

        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound[1]
        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')
        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')
        self.R = tf.placeholder(tf.float32, [None, 1], 'r')

        with tf.variable_scope('Actor'):
            self.a = self._build_a(self.S, scope='eval', trainable=True)
            a_ = self._build_a(self.S_, scope='target', trainable=False)
        with tf.variable_scope('Critic'):
            # assign self.a = a in memory when calculating q for td_error,
            # otherwise the self.a is from Actor when updating Actor
            q = self._build_c(self.S, self.a, scope='eval', trainable=True)
            q_ = self._build_c(self.S_, a_, scope='target', trainable=False)

        # networks parameters
        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')
        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')
        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')
        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')

        # target net replacement
        self.soft_replace = [[tf.assign(ta, (1 - TAU) * ta + TAU * ea), tf.assign(tc, (1 - TAU) * tc + TAU * ec)]
                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]

        q_target = self.R + GAMMA * q_
        # in the feed_dic for the td_error, the self.a should change to actions in memory
        td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)
        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, var_list=self.ce_params)

        a_loss = - tf.reduce_mean(q)    # maximize the q
        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)

        self.sess.run(tf.global_variables_initializer())

    def choose_action(self, s):
        return self.sess.run(self.a, {self.S: s[None, :]})[0]

    def learn(self):
        # soft target replacement
        self.sess.run(self.soft_replace)

        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)
        bt = self.memory[indices, :]
        bs = bt[:, :self.s_dim]
        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]
        br = bt[:, -self.s_dim - 1: -self.s_dim]
        bs_ = bt[:, -self.s_dim:]

        self.sess.run(self.atrain, {self.S: bs})
        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})

    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s, a, [r], s_))
        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory
        self.memory[index, :] = transition
        self.pointer += 1
        if self.pointer > MEMORY_CAPACITY:      # indicator for learning
            self.memory_full = True

    def _build_a(self, s, scope, trainable):
        with tf.variable_scope(scope):
            net = tf.keras.layers.Dense(
                300,
                activation='relu',
                trainable=trainable,
                name='l1'
            )(s)

            a = tf.keras.layers.Dense(
                self.a_dim,
                activation='tanh',
                trainable=trainable,
                name='a'
            )(net)

            return tf.multiply(a, self.a_bound, name='scaled_a')


    def _build_c(self, s, a, scope, trainable):
        with tf.variable_scope(scope):
            n_l1 = 300
            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)
            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)
            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)

            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)

            q = tf.keras.layers.Dense(
                1,
                trainable=trainable,
                name='q'
            )(net)

            return q


    def save(self):
        saver = tf.train.Saver()
        saver.save(self.sess, './params', write_meta_graph=False)

    def restore(self):
        saver = tf.train.Saver()
        saver.restore(self.sess, './params')"""

        with st.expander("é»æ“Šè¤‡è£½å®Œæ•´ç¨‹å¼ç¢¼ (rl.py)"):
            st.code(code, language="python")

        st.markdown("---")

        st.header('äº”ã€è¨“ç·´èˆ‡æ¸¬è©¦æµç¨‹')
        st.subheader("5.1 è¨“ç·´æ¨¡å¼")
        show_media("img/RL/rl_train.mp4","video")
        code = """
for episode:
    s = env.reset()
    for step:
        a = actor(s) + noise
        s_, r = env.step(a)
        buffer.store(s, a, r, s_)
        agent.learn()"""
        st.code(code, language="python")
        st.markdown("""
        * æ¯å€‹ episode éš¨æ©Ÿç”Ÿæˆç›®æ¨™ä½ç½®
        * åŠ å…¥æ¢ç´¢å™ªéŸ³ä¿ƒé€²æ¢ç´¢
        * é€é replay buffer æŒçºŒæ›´æ–°ç­–ç•¥
        """)

        st.markdown("---")

        st.subheader("5.2 æ¸¬è©¦èˆ‡å±•ç¤ºæ¨¡å¼")
        code = """
a = rl.choose_action(s)  # ç„¡ noise
env.render()"""
        st.code(code, language="python")
        st.caption("æ¸¬è©¦æ™‚ä»¥æ»‘é¼ å³æ™‚æ§åˆ¶ç›®æ¨™ä½ç½®ï¼Œç›´è§€å±•ç¤ºå­¸ç¿’å¾Œä¹‹è¿½è¹¤èƒ½åŠ›èˆ‡ç©©å®šæ€§ã€‚")

        st.markdown("---")

        st.header("å…­ã€å°ˆæ¡ˆç‰¹è‰²èˆ‡è²¢ç»")
        st.markdown("""
        <div style="font-size: 20px; line-height: 1.8;">
            <strong>ä¸ä¾è³´è§£æå¼é€†é‹å‹•å­¸</strong>
            <ol style="margin-top: 10px;">
                <li>æˆåŠŸå¯¦ç¾ä¸‰é€£æ¡¿æœ«ç«¯ä¹‹å³æ™‚è¿½è¹¤æ§åˆ¶</li>
                <li>çµåˆå¼·åŒ–å­¸ç¿’èˆ‡æ§åˆ¶å¾Œè™•ç†ï¼Œé¡¯è‘—é™ä½æŠ–å‹•</li>
                <li>æ¶æ§‹æ¨¡çµ„åŒ–ï¼Œæ˜“æ–¼æ“´å±•è‡³ï¼š
                    <ul style="list-style-type: circle; margin-left: 20px; margin-top: 5px;">
                        <li>å¤šé€£æ¡¿æ©Ÿæ¢°æ‰‹è‡‚</li>
                        <li>3D ç©ºé–“æ§åˆ¶</li>
                        <li>å¯¦é«”æ©Ÿæ¢°æ‰‹è‡‚å¹³å°</li>
                    </ul>
                </li>
            </ol>
        </div>
        """, unsafe_allow_html=True)

        st.markdown("---")

        st.header("ä¸ƒã€çµæœå±•ç¤º")
        show_media("img/RL/RL final.mp4", "video")
        code_main_py = """
# main.py
from env import ArmEnv
from rl import DDPG
import numpy as np

# ===============================
# è¨“ç·´ç›¸é—œåƒæ•¸è¨­å®šï¼ˆâ˜…é‡é»ï¼‰
# ===============================
MAX_EPISODES = 1200        # ä¸‰é€£æ¡¿è‡³å°‘è¦ 1000+
MAX_EP_STEPS = 400
ON_TRAIN = False            # â˜… ä¸€å®šè¦ True æ‰æœƒå­¸

# ===============================
# Training Function
# ===============================
def train():
    # ğŸ”¹ è¨“ç·´æ™‚ï¼šç›®æ¨™å›ºå®šåœ¨ä¸€å€‹ episode å…§
    # ğŸ”¹ æ¯å€‹ episode æ›ä¸€å€‹ç›®æ¨™ï¼ˆå­¸æ³›åŒ–ï¼‰
    env = ArmEnv(
        allow_mouse_goal=False,
        random_goal_on_reset=True
    )

    s_dim = env.state_dim
    a_dim = env.action_dim
    a_bound = env.action_bound

    rl = DDPG(a_dim, s_dim, a_bound)

    for i in range(MAX_EPISODES):
        s = env.reset()
        ep_r = 0.0

        for j in range(MAX_EP_STEPS):

            # -----------------------------
            # Actor + exploration noise
            # -----------------------------
            a = rl.choose_action(s)

            # â˜… noise ä¸è¦å¤ªå°ï¼Œä¸ç„¶å­¸ä¸åˆ°
            a = np.clip(
                np.random.normal(a, 0.15),
                -1, 1
            )

            # -----------------------------
            # Env step
            # -----------------------------
            s_, r, done = env.step(a)

            rl.store_transition(s, a, r, s_)
            ep_r += r
            s = s_

            if rl.memory_full:
                rl.learn()

            # â˜… æˆåŠŸå°±çµæŸ episode
            if done:
                print(f'Ep {i:04d} | DONE | ep_r={ep_r:.2f} | step={j}')
                break

            if j == MAX_EP_STEPS - 1:
                print(f'Ep {i:04d} | ---- | ep_r={ep_r:.2f}')

    rl.save()
    print('[INFO] Training finished & model saved.')

# ===============================
# Evaluation Function
# ===============================
def eval():
    # ğŸ”¹ æ¸¬è©¦æ™‚ï¼šç›®æ¨™è·Ÿæ»‘é¼ 
    env = ArmEnv(
        allow_mouse_goal=True,
        random_goal_on_reset=False
    )

    s_dim = env.state_dim
    a_dim = env.action_dim
    a_bound = env.action_bound

    rl = DDPG(a_dim, s_dim, a_bound)
    rl.restore()

    s = env.reset()
    while True:
        env.render()
        a = rl.choose_action(s)   # â˜… ä¸åŠ  noise
        s, r, done = env.step(a)

# ===============================
# ä¸»ç¨‹å¼å…¥å£
# ===============================
if ON_TRAIN:
    train()
else:
    eval()
"""
        with st.expander("é»æ“Šè¤‡è£½å®Œæ•´ç¨‹å¼ç¢¼ (main.py)"):
          st.code(code_main_py, language="python")

        code_env_py = """
# env.py
import numpy as np
import pyglet

class ArmEnv(object):
    viewer = None
    dt = 0.1
    action_bound = [-1, 1]
    action_dim = 3
    state_dim = 15  # dist_vec(2) + dist(1) + ee_vel(2) + on_goal(1) + cos(3)+sin(3) + prev_action(3)

    def __init__(self, allow_mouse_goal=False, random_goal_on_reset=True):
        self.W, self.H = 400, 400
        self.base = np.array([200., 200.], dtype=np.float32)

        self.goal = {'x': 100., 'y': 100., 'l': 50.0}

        self.allow_mouse_goal = allow_mouse_goal
        self.random_goal_on_reset = random_goal_on_reset

        self.arm_info = np.zeros((3, 2), dtype=np.float32)
        self.arm_info[:, 0] = [100.0, 100.0, 50.0]
        self.arm_info[:, 1] = np.random.uniform(0, 2*np.pi, size=3).astype(np.float32)

        self.on_goal = 0
        self.prev_action = np.zeros(3, dtype=np.float32)
        self.prev_ee_pos = self._get_ee_pos()
        self.prev_dist = self._dist_to_goal(self.prev_ee_pos)

    # ===================== Kinematics =====================
    def _get_joint_positions(self):
        tr = self.arm_info[:, 1]
        l = self.arm_info[:, 0]

        p0 = self.base.copy()
        p1 = p0 + np.array([np.cos(tr[0]), np.sin(tr[0])]) * l[0]
        p2 = p1 + np.array([np.cos(tr[0]+tr[1]), np.sin(tr[0]+tr[1])]) * l[1]
        p3 = p2 + np.array([np.cos(tr[0]+tr[1]+tr[2]), np.sin(tr[0]+tr[1]+tr[2])]) * l[2]
        return p0, p1, p2, p3

    def _get_ee_pos(self):
        return self._get_joint_positions()[-1]

    def _dist_to_goal(self, ee_pos):
        g = np.array([self.goal['x'], self.goal['y']], dtype=np.float32)
        return float(np.linalg.norm(ee_pos - g))

    def _get_state(self):
        tr = self.arm_info[:, 1]
        ee = self._get_ee_pos()
        g = np.array([self.goal['x'], self.goal['y']], dtype=np.float32)

        dist_vec = (g - ee) / 200.0
        dist = np.linalg.norm(g - ee) / 200.0
        ee_vel = (ee - self.prev_ee_pos) / 20.0
        touch = 1.0 if self.on_goal > 0 else 0.0
        c = np.cos(tr)
        s = np.sin(tr)

        return np.concatenate([
            dist_vec, [dist], ee_vel, [touch], c, s, self.prev_action
        ]).astype(np.float32)

    # ===================== RL Interface =====================
    def step(self, action):
        done = False

        # --- clip ---
        action = np.clip(action, -0.5, 0.5).astype(np.float32)

        ee = self._get_ee_pos()
        dist = self._dist_to_goal(ee)

        # === 1ï¸âƒ£ è·é›¢è‡ªé©æ‡‰å¹³æ»‘ï¼ˆè¶Šè¿‘è¶Šç©©ï¼‰===
        if dist < self.goal['l']:
            alpha = 0.9
        else:
            alpha = 0.75
        action = alpha * self.prev_action + (1 - alpha) * action

        # === 2ï¸âƒ£ å¾®å°è§’é€Ÿåº¦æ­»å€ï¼ˆæŠ–å‹•æ®ºæ‰‹ï¼‰===
        rate_deadband = 0.01
        action[np.abs(action) < rate_deadband] = 0.0

        # === 3ï¸âƒ£ ç›®æ¨™å€ soft stop ===
        if dist < self.goal['l'] * 0.5:
            action *= 0.2

        # --- update joint ---
        self.arm_info[:, 1] += action * self.dt
        self.arm_info[:, 1] %= (2 * np.pi)

        ee_new = self._get_ee_pos()
        dist_new = self._dist_to_goal(ee_new)

        # --- reward ---
        r = (self.prev_dist - dist_new) * 20.0
        self.prev_dist = dist_new

        ee_vel = np.linalg.norm(ee_new - self.prev_ee_pos)
        r -= 0.01 * ee_vel
        r -= 0.001 * np.sum(np.abs(action))

        if dist_new < self.goal['l']:
            r += 5.0
            self.on_goal += 1
            if self.on_goal >= 50:
                done = True
                r += 50.0
        else:
            self.on_goal = 0

        self.prev_action = action.copy()
        self.prev_ee_pos = ee_new.copy()

        return self._get_state(), float(r), done

    def reset(self):
        self.on_goal = 0
        self.prev_action = np.zeros(3, dtype=np.float32)
        self.arm_info[:, 1] = np.random.uniform(0, 2*np.pi, size=3).astype(np.float32)

        if self.random_goal_on_reset and not self.allow_mouse_goal:
            margin = 70
            self.goal['x'] = float(np.random.uniform(margin, self.W - margin))
            self.goal['y'] = float(np.random.uniform(margin, self.H - margin))

        self.prev_ee_pos = self._get_ee_pos()
        self.prev_dist = self._dist_to_goal(self.prev_ee_pos)

        return self._get_state()

    def render(self):
        if self.viewer is None:
            self.viewer = Viewer(self.arm_info, self.goal, self.base, self.allow_mouse_goal, self.W, self.H)
        self.viewer.render()

    def sample_action(self):
        return np.random.uniform(-0.5, 0.5, size=3).astype(np.float32)


# ===================== Viewer =====================
class Viewer(pyglet.window.Window):
    bar_thc = 5

    def __init__(self, arm_info, goal, base, allow_mouse_goal, W, H):
        super().__init__(width=W, height=H, caption='Arm')
        pyglet.gl.glClearColor(1, 1, 1, 1)

        self.arm_info = arm_info
        self.goal_info = goal
        self.base = base
        self.allow_mouse_goal = allow_mouse_goal
        self.batch = pyglet.graphics.Batch()

        self.point = self.batch.add(4, pyglet.gl.GL_QUADS, None,
            ('v2f', [0]*8), ('c3B', (86, 109, 249)*4))

        self.arm1 = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', [0]*8), ('c3B', (249, 86, 86)*4))
        self.arm2 = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', [0]*8), ('c3B', (249, 86, 86)*4))
        self.arm3 = self.batch.add(4, pyglet.gl.GL_QUADS, None, ('v2f', [0]*8), ('c3B', (249, 86, 86)*4))

    def render(self):
        self._update()
        self.switch_to()
        self.dispatch_events()
        self.clear()
        self.batch.draw()
        self.flip()

    def _update(self):
        x, y, l = self.goal_info['x'], self.goal_info['y'], self.goal_info['l']
        self.point.vertices = [
            x-l/2, y-l/2, x-l/2, y+l/2, x+l/2, y+l/2, x+l/2, y-l/2
        ]

        a1l, a2l, a3l = self.arm_info[:, 0]
        a1r, a2r, a3r = self.arm_info[:, 1]
        p0 = self.base
        p1 = p0 + np.array([np.cos(a1r), np.sin(a1r)]) * a1l
        p2 = p1 + np.array([np.cos(a1r+a2r), np.sin(a1r+a2r)]) * a2l
        p3 = p2 + np.array([np.cos(a1r+a2r+a3r), np.sin(a1r+a2r+a3r)]) * a3l

        def quad(pA, pB):
            v = pB - pA
            v = v / (np.linalg.norm(v) + 1e-6)
            n = np.array([-v[1], v[0]]) * self.bar_thc
            return np.concatenate([pA-n, pA+n, pB+n, pB-n]).astype(int).tolist()

        self.arm1.vertices = quad(p0, p1)
        self.arm2.vertices = quad(p1, p2)
        self.arm3.vertices = quad(p2, p3)

    def on_mouse_motion(self, x, y, dx, dy):
        if self.allow_mouse_goal:
            self.goal_info['x'] = float(x)
            self.goal_info['y'] = float(y)"""

        with st.expander("é»æ“Šè¤‡è£½å®Œæ•´ç¨‹å¼ç¢¼ (env.py)"):
            st.code(code_env_py, language="python")

        code_rl_py = """
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np

#####################  hyper parameters  ####################

LR_A = 0.001    # learning rate for actor
LR_C = 0.001    # learning rate for critic
GAMMA = 0.9     # reward discount
TAU = 0.01      # soft replacement
MEMORY_CAPACITY = 30000
BATCH_SIZE = 32


class DDPG(object):
    def __init__(self, a_dim, s_dim, a_bound,):
        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)
        self.pointer = 0
        self.memory_full = False
        self.sess = tf.Session()
        self.a_replace_counter, self.c_replace_counter = 0, 0

        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound[1]
        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')
        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')
        self.R = tf.placeholder(tf.float32, [None, 1], 'r')

        with tf.variable_scope('Actor'):
            self.a = self._build_a(self.S, scope='eval', trainable=True)
            a_ = self._build_a(self.S_, scope='target', trainable=False)
        with tf.variable_scope('Critic'):
            # assign self.a = a in memory when calculating q for td_error,
            # otherwise the self.a is from Actor when updating Actor
            q = self._build_c(self.S, self.a, scope='eval', trainable=True)
            q_ = self._build_c(self.S_, a_, scope='target', trainable=False)

        # networks parameters
        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')
        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')
        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')
        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')

        # target net replacement
        self.soft_replace = [[tf.assign(ta, (1 - TAU) * ta + TAU * ea), tf.assign(tc, (1 - TAU) * tc + TAU * ec)]
                             for ta, ea, tc, ec in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]

        q_target = self.R + GAMMA * q_
        # in the feed_dic for the td_error, the self.a should change to actions in memory
        td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)
        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, var_list=self.ce_params)

        a_loss = - tf.reduce_mean(q)    # maximize the q
        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)

        self.sess.run(tf.global_variables_initializer())

    def choose_action(self, s):
        return self.sess.run(self.a, {self.S: s[None, :]})[0]

    def learn(self):
        # soft target replacement
        self.sess.run(self.soft_replace)

        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)
        bt = self.memory[indices, :]
        bs = bt[:, :self.s_dim]
        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]
        br = bt[:, -self.s_dim - 1: -self.s_dim]
        bs_ = bt[:, -self.s_dim:]

        self.sess.run(self.atrain, {self.S: bs})
        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})

    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s, a, [r], s_))
        index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory
        self.memory[index, :] = transition
        self.pointer += 1
        if self.pointer > MEMORY_CAPACITY:      # indicator for learning
            self.memory_full = True

    def _build_a(self, s, scope, trainable):
        with tf.variable_scope(scope):
            net = tf.keras.layers.Dense(
                300,
                activation='relu',
                trainable=trainable,
                name='l1'
            )(s)

            a = tf.keras.layers.Dense(
                self.a_dim,
                activation='tanh',
                trainable=trainable,
                name='a'
            )(net)

            return tf.multiply(a, self.a_bound, name='scaled_a')


    def _build_c(self, s, a, scope, trainable):
        with tf.variable_scope(scope):
            n_l1 = 300
            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)
            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)
            b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)

            net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)

            q = tf.keras.layers.Dense(
                1,
                trainable=trainable,
                name='q'
            )(net)

            return q


    def save(self):
        saver = tf.train.Saver()
        saver.save(self.sess, './params', write_meta_graph=False)

    def restore(self):
        saver = tf.train.Saver()
        saver.restore(self.sess, './params')"""

        with st.expander("é»æ“Šè¤‡è£½å®Œæ•´ç¨‹å¼ç¢¼ (rl.py)"):
            st.code(code_rl_py, language="python")


    elif rl_nav == "çå‹µå‡½æ•¸è©³ç´°è§£èªª":
        st.title("çå‹µå‡½æ•¸è¨­è¨ˆï¼ˆReward Function Designï¼‰")

        st.header("ä¸€ã€è¨­è¨ˆç›®æ¨™èˆ‡åŸå‰‡")
        st.markdown("""
        åœ¨ä¸‰é€£æ¡¿æ©Ÿæ¢°æ‰‹è‡‚çš„é€£çºŒæ§åˆ¶å•é¡Œä¸­ï¼Œçå‹µå‡½æ•¸çš„è¨­è¨ˆå°æ–¼å­¸ç¿’æ•ˆç‡èˆ‡ç­–ç•¥ç©©å®šæ€§å…·æœ‰æ±ºå®šæ€§å½±éŸ¿ã€‚æœ¬ç ”ç©¶åœ¨è¨­è¨ˆçå‹µå‡½æ•¸æ™‚ï¼Œéµå¾ªä»¥ä¸‹åŸå‰‡ï¼š

        1. **é¿å… Sparse Reward å•é¡Œ**
           * è‹¥åƒ…åœ¨æˆåŠŸåˆ°é”ç›®æ¨™æ™‚çµ¦äºˆçå‹µï¼Œå°‡å°è‡´å­¸ç¿’åˆæœŸå›é¥‹æ¥µåº¦ç¨€ç–ï¼Œç­–ç•¥é›£ä»¥æ”¶æ–‚ã€‚

        2. **æä¾›é€£çºŒä¸”å…·æ–¹å‘æ€§çš„å›é¥‹**
           * Agent éœ€è¦å³æ™‚çŸ¥é“ã€Œç›®å‰å‹•ä½œæ˜¯å¦æœæ­£ç¢ºæ–¹å‘å‰é€²ã€ï¼Œè€Œéåƒ…å¾—çŸ¥çµæœå¥½å£ã€‚

        3. **å…¼é¡§ç©©å®šæ€§èˆ‡å¹³æ»‘æ€§**
           * é™¤äº†åˆ°é”ç›®æ¨™ï¼Œäº¦éœ€é¿å…é«˜é »éœ‡ç›ªèˆ‡å¤šé¤˜é—œç¯€é‹å‹•ï¼Œä½¿è¡Œç‚ºæ›´ç¬¦åˆå¯¦éš›æ©Ÿæ¢°æ‰‹è‡‚ä¹‹æ§åˆ¶éœ€æ±‚ã€‚

        åŸºæ–¼ä¸Šè¿°è€ƒé‡ï¼Œæœ¬ç ”ç©¶æ¡ç”¨è·é›¢å·®åˆ†å¼ï¼ˆDistance Difference-basedï¼‰çå‹µè¨­è¨ˆï¼Œä¸¦æ­é…å¤šé …æ‡²ç½°èˆ‡æˆåŠŸçå‹µé …ã€‚
        """)

        st.markdown("---")

        st.header("äºŒã€æ ¸å¿ƒçå‹µé …ï¼šè·é›¢å·®åˆ†çå‹µ")
        st.subheader("2.1 å®šç¾©æ–¹å¼")
        st.markdown("ä¸»è¦çå‹µé …å®šç¾©ç‚ºæœ«ç«¯åŸ·è¡Œå™¨åˆ°ç›®æ¨™è·é›¢çš„è®ŠåŒ–é‡ï¼š")
        st.latex(r"R_{dist} = (d_{t-1} - d_t) \times k")
        st.markdown("""
        å…¶ä¸­ï¼š
        * $d_{t-1}$ï¼šå‰ä¸€æ™‚é–“æ­¥æœ«ç«¯èˆ‡ç›®æ¨™ä¹‹è·é›¢ (`prev_dist`)
        * $d_t$ï¼šç›®å‰æ™‚é–“æ­¥æœ«ç«¯èˆ‡ç›®æ¨™ä¹‹è·é›¢ (`current_dist`)
        * $k$ï¼šè·é›¢ç¸®æ”¾ä¿‚æ•¸ï¼ˆæœ¬ç ”ç©¶è¨­å®šç‚º 20ï¼‰
        """)

        st.subheader("2.2 è¨­è¨ˆå‹•æ©Ÿ")
        st.markdown("""
        æ­¤è¨­è¨ˆå…·å‚™ä»¥ä¸‹ç‰¹æ€§ï¼š
        * è‹¥æœ«ç«¯é è¿‘ç›®æ¨™ ($d_t < d_{t-1}$)ï¼Œå‰‡ç²å¾— **æ­£çå‹µ**
        * è‹¥æœ«ç«¯é é›¢ç›®æ¨™ï¼Œå‰‡ç²å¾— **è² çå‹µ**
        * ç•¶æœ«ç«¯åœæ»¯ä¸å‹•æ™‚ï¼Œçå‹µè¶¨è¿‘æ–¼ 0

        ç›¸è¼ƒæ–¼ç›´æ¥ä½¿ç”¨ `-distance` ä½œç‚º rewardï¼Œæ­¤æ–¹å¼èƒ½ï¼š
        * æä¾›æ›´æ˜ç¢ºçš„ã€Œæ–¹å‘æ€§æ¢¯åº¦ã€
        * é¿å…åœ¨è·é›¢å¾ˆé æ™‚ reward è®ŠåŒ–éå°è€Œå°è‡´å­¸ç¿’åœæ»¯
        * æå‡å­¸ç¿’åˆæœŸçš„æ¢ç´¢æ•ˆç‡

        å› æ­¤ï¼Œæ­¤è·é›¢å·®åˆ†å¼çå‹µèƒ½æœ‰æ•ˆå¼•å° Agent é€æ­¥å­¸ç¿’æœå‘ç›®æ¨™ç§»å‹•çš„ç­–ç•¥ã€‚
        """)

        st.markdown("---")

        st.header("ä¸‰ã€ç©©å®šæ€§ç›¸é—œæ‡²ç½°é …ï¼ˆStability-related Penaltiesï¼‰")

        st.subheader("3.1 æœ«ç«¯é€Ÿåº¦æ‡²ç½°ï¼ˆEnd-effector Velocity Penaltyï¼‰")
        st.markdown("ç‚ºé¿å…æœ«ç«¯åœ¨æ¥è¿‘ç›®æ¨™æ™‚ç”¢ç”Ÿé«˜é »éœ‡ç›ªï¼Œæœ¬ç ”ç©¶å¼•å…¥æœ«ç«¯é€Ÿåº¦æ‡²ç½°é …ï¼š")
        st.latex(r"R_{vel} = - \lambda_v \cdot \| \mathbf{v}_{ee} \|")
        st.markdown("""
        å…¶ä¸­ï¼š
        * $\mathbf{v}_{ee}$ ä»£è¡¨æœ«ç«¯åŸ·è¡Œå™¨çš„é€Ÿåº¦å‘é‡ (æˆ– $\| ee_t - ee_{t-1} \|$)
        * $\lambda_v$ ç‚ºæ¬Šé‡ä¿‚æ•¸ï¼ˆæœ¬ç ”ç©¶è¨­å®šç‚º 0.01ï¼‰

        **è¨­è¨ˆæ„ç¾©**
        * æŠ‘åˆ¶æœ«ç«¯åœ¨ç›®æ¨™é™„è¿‘ä¾†å›æ“ºå‹•
        * é¼“å‹µå¹³æ»‘ä¸”é€£çºŒçš„é‹å‹•è»Œè·¡
        * é¿å…ç­–ç•¥é€éã€ŒåŠ‡çƒˆä¿®æ­£ã€ä¾†æ›å–å¾®å°è·é›¢æ”¹å–„

        æ­¤æ‡²ç½°é …ä½¿ Agent åœ¨å­¸ç¿’éç¨‹ä¸­è‡ªç„¶åå¥½ä½é€Ÿåº¦ã€ç©©å®šæ”¶æ–‚çš„æ§åˆ¶è¡Œç‚ºã€‚
        """)

        st.subheader("3.2 é—œç¯€è§’é€Ÿåº¦æ‡²ç½°ï¼ˆAction Magnitude Penaltyï¼‰")
        st.markdown("ç‚ºé€²ä¸€æ­¥æ¸›å°‘ä¸å¿…è¦çš„é—œç¯€å‹•ä½œï¼Œæœ¬ç ”ç©¶å°å‹•ä½œå¤§å°åŠ å…¥æ‡²ç½°ï¼š")
        st.latex(r"R_{action} = - \lambda_a \cdot \sum_i |a_i|")
        st.markdown("""
        å…¶ä¸­ï¼š
        * $a_i$ ç‚ºç¬¬ i å€‹é—œç¯€ä¹‹è§’é€Ÿåº¦æ§åˆ¶é‡
        * $\lambda_a$ ç‚ºæ‡²ç½°æ¬Šé‡ï¼ˆæœ¬ç ”ç©¶è¨­å®šç‚º 0.001ï¼‰

        **è¨­è¨ˆæ„ç¾©**
        * é™ä½é—œç¯€åœ¨ç›®æ¨™é™„è¿‘çš„é«˜é »å°å¹…éœ‡ç›ª
        * å¼•å° Actor å­¸ç¿’ã€Œèƒ½ä¸å‹•å°±ä¸å‹•ã€çš„ç­–ç•¥
        * æå‡æ•´é«”æ§åˆ¶çš„èƒ½æºæ•ˆç‡èˆ‡å¹³é †æ€§

        æ­¤é …åœ¨ä¸å½±éŸ¿è¿½è¹¤èƒ½åŠ›çš„å‰æä¸‹ï¼Œæœ‰æ•ˆæ”¹å–„è¦–è¦ºä¸Šçš„æŠ–å‹•å•é¡Œã€‚
        """)

        st.markdown("---")

        st.header("å››ã€æˆåŠŸèˆ‡çµ‚æ­¢çå‹µï¼ˆSuccess and Termination Rewardï¼‰")
        st.subheader("4.1 é€²å…¥ç›®æ¨™å€åŸŸçå‹µ")
        st.markdown("ç•¶æœ«ç«¯é€²å…¥ç›®æ¨™åŠå¾‘ç¯„åœå…§æ™‚ï¼Œçµ¦äºˆå³æ™‚çå‹µï¼š")
        st.latex(r"R_{in\_goal} = +5.0")
        st.markdown("""
        æ­¤è¨­è¨ˆèƒ½ï¼š
        * æ˜ç¢ºå‘ŠçŸ¥ Agentã€Œå·²é”åˆ°ç›®æ¨™ã€
        * åŠ é€Ÿç­–ç•¥æ”¶æ–‚è‡³ç›®æ¨™é™„è¿‘
        """)

        st.subheader("4.2 ç©©å®šåœç•™çå‹µèˆ‡ Episode çµ‚æ­¢")
        st.markdown("ç‚ºé¿å…ç­–ç•¥åƒ…çŸ­æš«è§¸ç¢°ç›®æ¨™å¾Œé›¢é–‹ï¼Œæœ¬ç ”ç©¶é€²ä¸€æ­¥è¨­è¨ˆæŒçºŒå‘½ä¸­æ©Ÿåˆ¶ï¼š")
        st.markdown("""
        è‹¥é€£çºŒ **N** æ­¥åœç•™åœ¨ç›®æ¨™å€ï¼š
        """)
        st.latex(r"R_{terminal} = +r_{success}")
        st.markdown("""
        å…¶ä¸­ï¼š
        * $N$ ç‚ºé€£çºŒåœç•™æ­¥æ•¸ï¼ˆæœ¬ç ”ç©¶è¨­å®šç‚º 50ï¼‰
        * $r_{success}$ ç‚ºæˆåŠŸçµ‚æ­¢çå‹µï¼ˆæœ¬ç ”ç©¶è¨­å®šç‚º 50ï¼‰

        **è¨­è¨ˆæ„ç¾©**
        * é¼“å‹µæœ«ç«¯ã€Œç©©å®šåœç•™ã€è€ŒéçŸ­æš«ç¢°è§¸
        * å¼·åŒ–é•·æœŸç©©å®šæ§åˆ¶è¡Œç‚º
        * æä¾›æ˜ç¢º episode çµæŸæ¢ä»¶ï¼Œæœ‰åŠ©æ–¼ç­–ç•¥æ”¶æ–‚
        """)

        st.markdown("---")

        st.header("äº”ã€æ•´é«”çå‹µå‡½æ•¸ç¸½çµ")
        st.markdown("ç¶œåˆä¸Šè¿°è¨­è¨ˆï¼Œæœ¬ç ”ç©¶ä¹‹çå‹µå‡½æ•¸å¯è¡¨ç¤ºç‚ºï¼š")
        st.latex(r"R_{total} = R_{dist} + R_{vel} + R_{action} + R_{in\_goal} + R_{terminal}")
        st.markdown("""
        æ­¤çå‹µå‡½æ•¸åŒæ™‚å…¼é¡§ï¼š
        1. **å¼•å°æ€§ï¼ˆGuidanceï¼‰**ï¼šè·é›¢å·®åˆ†é …
        2. **ç©©å®šæ€§ï¼ˆStabilityï¼‰**ï¼šé€Ÿåº¦èˆ‡å‹•ä½œæ‡²ç½°
        3. **æˆåŠŸæ€§ï¼ˆSuccessï¼‰**ï¼šç›®æ¨™å‘½ä¸­èˆ‡çµ‚æ­¢çå‹µ

        ä½¿å¾— Agent èƒ½åœ¨é€£çºŒæ§åˆ¶å•é¡Œä¸­ï¼Œå­¸ç¿’åˆ°å…¼å…·æº–ç¢ºæ€§èˆ‡å¹³é †æ€§çš„æ§åˆ¶ç­–ç•¥ã€‚
        """)

        st.markdown("---")

        st.header("å…­ã€è¨­è¨ˆæˆæ•ˆèªªæ˜ï¼ˆå¯¦é©—è§€å¯Ÿï¼‰")
        st.markdown("""
        å¯¦é©—çµæœé¡¯ç¤ºï¼Œé€éä¸Šè¿°çå‹µè¨­è¨ˆï¼š
        * å­¸ç¿’åˆæœŸèƒ½å¿«é€Ÿå­¸æœƒæœç›®æ¨™ç§»å‹•
        * ä¸­å¾ŒæœŸèƒ½ç©©å®šåœç•™æ–¼ç›®æ¨™å€åŸŸ
        * é…åˆæ§åˆ¶å¾Œè™•ç†å¾Œï¼Œå¯é¡¯è‘—é™ä½é«˜é »æŠ–å‹•ç¾è±¡

        é¡¯ç¤ºæœ¬çå‹µå‡½æ•¸è¨­è¨ˆé©åˆæ‡‰ç”¨æ–¼é€£çºŒå‹æ©Ÿæ¢°æ‰‹è‡‚æ§åˆ¶å•é¡Œã€‚
        """)

elif menu == "TurtleBot Burgerå¹³å°":
    st.title("TurtleBot Burgerå¹³å°")
    st.markdown("---")

    st.header("ä¸€ã€å¯¦ä½œéç¨‹ï¼šé¿éšœèˆ‡å°èˆª")
    st.subheader('æ­¥é©Ÿ 1ï¼šVirtualBox è¨­å®š')
    show_media("img/Turtlebot/1.jpg")

    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 2ï¼šç¶²è·¯è¨­å®šèˆ‡å»ºç«‹å·¥ä½œç©ºé–“')
    col_tb2_1, col_tb2_2 = st.columns(2)
    with col_tb2_1:
        show_media("img/Turtlebot/2.jpg")
    with col_tb2_2:
        show_media("img/Turtlebot/2-2.jpg")

    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 3ï¼šå®‰è£ Turtlebot3 å¥—ä»¶')
    show_media("img/Turtlebot/3.jpg")
    st.code("git clone https://github.com/ROBOTIS-GIT/turtlebot3\ncd ..\ncatkin_make", language="bash")

    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 4ï¼šå•Ÿå‹• ROS Core èˆ‡é€£æ¥ Turtlebot3')
    show_media("img/Turtlebot/ros core.jpg")
    st.code("source /opt/ros/noetic/setup.bash\nsource ~/mde_ws/devel_isolated/setup.bash\nroscore", language="bash")

    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 5ï¼šå•Ÿå‹• SLAM å»ºåœ–')
    show_media("img/Turtlebot/slam.jpg")
    st.code("export TURTLEBOT3_MODEL=burger\nroslaunch turtlebot3_slam turtlebot3_slam.launch", language="bash")

    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 6ï¼šæƒæåœ°å½¢')
    show_media("img/Turtlebot/map.jpg")
    show_media("img/Turtlebot/real.jpg")


    st.markdown("---")

    st.subheader('æ­¥é©Ÿ 7ï¼šå„²å­˜èˆ‡é–‹å•Ÿåœ°åœ–')
    col_tb11_1, col_tb11_2 = st.columns(2)
    with col_tb11_1:
        show_media("img/Turtlebot/save map.jpg")
    with col_tb11_2:
        show_media("img/Turtlebot/save map 2.jpg")
    st.code("rosrun map_server map_saver -f ~/mde_ws/map00", language="bash")

    st.markdown("---")

    st.header("äºŒã€è·¯å¾‘è¦åŠƒèˆ‡çµæœå±•ç¤º")
    st.markdown('### æˆåŠŸå°èˆªä½¿ Turtlebot3 åˆ°ç›®çš„åœ°ï¼Œä¸¦ä¸”é¿é–‹éšœç¤™ç‰©')
    show_media("img/Turtlebot/road.jpg")

    st.markdown("---")

    show_media("img/Turtlebot/final.mp4", "video")

elif menu == "Streamlit UIè¨­è¨ˆèˆ‡è³‡æ–™å¯è¦–åŒ–":
    st.title("Streamlit UI è¨­è¨ˆèˆ‡è³‡æ–™å¯è¦–åŒ–")
    st.subheader("æ­¥é©Ÿ 1ï¼šå®‰è£ streamlit å¥—ä»¶")
    st.code("pip install streamlit", language="bash")

    st.subheader("æ­¥é©Ÿ 2ï¼šå•Ÿå‹• streamlitï¼Œé–‹å•Ÿç¶²é ")
    st.code("streamlit run app.py", language="bash")

    st.subheader("æ­¥é©Ÿ 3ï¼šé–‹å•Ÿå®˜ç¶²ç¨‹å¼åº«")
    st.markdown("é–‹å•Ÿ [ğŸ”—Streamlit Cheat Sheet](https://cheat-sheet.streamlit.app/)ï¼Œå¾ä¸­å¯ç²çš„å„ç¨®ç¨‹å¼ä»¥ä¾›ç¶²é æ›¸å¯«")